# 🚀 3分钟极速启动 (Quick Start)

> **核心目标：** 无论官网是否崩了，你都能通过以下两种方式（本地部署 / 云端平替）立即用上 DeepSeek R1。

---

## 💻 方案一：本地部署（永久免费、隐私安全）

只要你有一台电脑（Windows / Mac / Linux），就能把 DeepSeek 装进硬盘里。

### 第一步：安装 Ollama (AI 启动器)
请直接访问官网下载安装包，一路“下一步”安装即可。
* 👉 **下载地址：** [https://ollama.com/download](https://ollama.com/download)

### 第二步：根据你的电脑配置，复制对应命令
打开你的 **终端 (Mac/Linux)** 或 **PowerShell (Windows)**，根据你的内存大小，复制下方**【对应的】**一条命令并回车：

| 你的电脑内存 | 推荐模型版本 | 适用场景 | **🚀 复制这条命令 (右键粘贴)** |
| :--- | :--- | :--- | :--- |
| **8GB** | **1.5b** (极速版) | 尝鲜、简单对话、老旧电脑 | `ollama run deepseek-r1:1.5b` |
| **16GB** | **7b / 8b** (标准版) | 绝大多数人的首选，平衡最好 | `ollama run deepseek-r1:7b` |
| **32GB+** | **14b / 32b** (进阶版) | 复杂逻辑推理、长文写作 | `ollama run deepseek-r1:14b` |

> **注意：** 第一次运行会自动下载模型文件（约 1.5G - 10G 不等），请耐心等待进度条走完。

### 第三步：开始对话
当屏幕出现 `>>>` 符号时，说明部署成功！直接输入中文即可开始对话。
* **退出对话：** 输入 `/bye` 或按 `Ctrl + D`。

---

## ☁️ 方案二：官网替代方案（免显卡、在线用）

如果官网 (deepseek.com) 显示“服务器繁忙”，请立即切换到以下 **“满血版”** 镜像站：

1.  **硅基流动 (SiliconFlow)**
    * **推荐理由：** 国内最快的 DeepSeek R1 API 服务商之一，注册通常送额度，速度极快。
    * 👉 [点击访问](https://cloud.siliconflow.cn/)

2.  **HuggingFace Chat**
    * **推荐理由：** 开源社区官方托管，免费使用 Qwen/DeepSeek 等模型（需科学网络环境）。
    * 👉 [点击访问](https://huggingface.co/chat/)

3.  **秘塔 AI 搜索 (Metaso)**
    * **推荐理由：** 如果你是为了搜资料、写论文，它内置了深度理解能力，无需排队。
    * 👉 [点击访问](https://metaso.cn/)

---

## 🛠️ 避坑指南 (Troubleshooting)

### Q1: Ollama 下载进度条不动 / 速度太慢？
* **原因：** 官方下载源在海外，网络波动。
* **解决：**
    1.  尝试在早上或深夜下载（避开高峰）。
    2.  或者使用下载加速器（迅雷等）手动下载模型文件（GGUF格式），然后导入 Ollama（*详见仓库主页进阶教程*）。

### Q2: 运行后电脑巨卡，或者报错 `out of memory`？
* **原因：** 你选的模型太大了，显存/内存爆了。
* **解决：**
    1.  按 `Ctrl + C` 强制停止。
    2.  **降级模型：** 比如你是 16G 内存跑 14b 卡顿，请换成 7b 版本。
    3.  复制命令：`ollama run deepseek-r1:7b` (它会自动下载更小的版本覆盖运行)。

---

> **觉得好用？** 请点击右上角 **Star ⭐** 收藏，防止下次找不到命令！